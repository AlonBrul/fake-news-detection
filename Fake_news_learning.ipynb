{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake news learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJIjcNfaYUtJ"
      },
      "source": [
        "In this notebook we classify fake news with a transformer model.<br>\n",
        "<br>\n",
        "We have already encoded the fake news train & test datasets in the **Fake news encoding** notebook,<br>\n",
        "here we will:\n",
        "- Add positional encodings to the data.\n",
        "- Initialize a transformer model.\n",
        "- Train the model on the train data.\n",
        "- Evaluate the models predictions on the test data.\n",
        "\n",
        "We show several versions of transfomer models and encodings of the datasets, in each version we try different approaches of encodings and model-building.\n",
        "\n",
        "\n",
        "A guide to Transformer models that we used:<br>\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ6CWuGpUNTL"
      },
      "source": [
        "# Learning with Bag-of-words encodings\n",
        "\n",
        "In this section, we use the corpus BOW encodings we created in the **Fake news encoding** notebook as data for our model.<br>\n",
        "<br>\n",
        "The corpus is encoded in the following manner- Each document is converted to an array of numbers, each number represents a matching word in the document. there are **56668** words altogether.<br>\n",
        "<br>\n",
        "We will build, fit and evaluate three different models:\n",
        "1. Transformer model that **multiclass-classifies** **6** categories.\n",
        "2. Transformer model that **binary-classifies**, fitted and tested only on documents that are labeled as **'true'**, **'false'**.\n",
        "3. Transformer model that **binary-classifies**, fitted and tested on all documents but the labels will be merged to two categories - **'true'**, **'false'**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOseeFb3mamo"
      },
      "source": [
        "## Model 1 - Multiclass classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Ng-03PUqqm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vUbvB1TVMCC"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD3Cx8YZUZWq"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train encoded\n",
        "num_splits = 4\n",
        "train_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_{i}.csv')\n",
        "  train_encoded = train_encoded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test encoded\n",
        "test_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_encoded_v3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmSCM2RVJuY"
      },
      "source": [
        "train_encoded_lst = list(train_encoded['full_text_encoded'])\n",
        "test_encoded_lst = list(test_encoded['full_text_encoded'])\n",
        "\n",
        "# documents encodings are saved as strings\n",
        "# use eval to return it to actual encodings (list of numbers)\n",
        "for i, doc in enumerate(train_encoded_lst):\n",
        "  train_encoded_lst[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_encoded_lst):\n",
        "  test_encoded_lst[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvNNVTONVS_L"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdHrVpQVJuY"
      },
      "source": [
        "train_labels_oh = pd.get_dummies(train['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxDPd4ofgeC"
      },
      "source": [
        "### Truncate/pad sequences to a *max sequence length*\n",
        "First we need to determine the value of *max sequence length*.<br>\n",
        "Find minimum, maximum and average sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz5H4yxcfSIM",
        "outputId": "3ac9affb-02a4-42da-9058-cca2835b5847"
      },
      "source": [
        "doc_len = len(train_encoded_lst[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in train_encoded_lst:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 33\n",
            "max sequence length = 1616\n",
            "avrage sequence length = 466.2858263541781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJXrULPKhoQq"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9YgE3MRpJyc"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TzsZybqc0JK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca1b866-1560-4fd8-fa60-34fd5b6584d8"
      },
      "source": [
        "maxlen = 500 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in tqdm(train_encoded_lst, position=0, leave=True):\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in tqdm(test_encoded_lst, position=0, leave=True):\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15052/15052 [00:00<00:00, 29457.99it/s]\n",
            "100%|██████████| 1266/1266 [00:00<00:00, 44658.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5q7ZTqbWWSk",
        "outputId": "83fad54e-de72-442a-fbd3-bdbdb050db37"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in test_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{500}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3juxAiXrXQbW"
      },
      "source": [
        "### Transformer blocks and positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys9r8ScIXQbX"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZVQ1IpWX40g"
      },
      "source": [
        "def positional_embeddings(maxlen):\n",
        "  pe_lst = []\n",
        "  for pos in range(maxlen):\n",
        "    pe_lst.append(sin(pos))\n",
        "\n",
        "  return pe_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsmo5ydBdMfe"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, maxlen):\n",
        "        positions = tf.convert_to_tensor(positional_embeddings(maxlen))\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz-D0DiQXQbX"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdrIAKYnXQbX"
      },
      "source": [
        "vocab_size = 56668 # taken from 'Fake news embedding' notebook\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs, maxlen)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhxDxBwiWuh6"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIChh-WXXQbY",
        "outputId": "0a297c6d-fd3f-4ff9-b0a2-aad217e3aa60"
      },
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"categorical_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "377/377 [==============================] - 233s 614ms/step - loss: 1.7644 - categorical_accuracy: 0.2060 - recall_20: 6.4111e-04 - val_loss: 1.6524 - val_categorical_accuracy: 0.2783 - val_recall_20: 0.0442\n",
            "Epoch 2/2\n",
            "377/377 [==============================] - 233s 618ms/step - loss: 1.5899 - categorical_accuracy: 0.3064 - recall_20: 0.0440 - val_loss: 1.5615 - val_categorical_accuracy: 0.3318 - val_recall_20: 0.0571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u44TIO8Fmap"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl16l0SfbO7O"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qileMuaua0mA",
        "outputId": "7a7ff93b-f062-4571-ce45-36f7b91e681f"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 7s 178ms/step - loss: 1.5917 - categorical_accuracy: 0.2930 - recall_20: 0.0134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvviP0xvl92s"
      },
      "source": [
        "## Model 2 - Binary classification, only 'true', 'false'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyVDeW0mlyvr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFjgb5u6lyvs"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZUGYdLAlyvs"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train encoded\n",
        "num_splits = 4\n",
        "train_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_{i}.csv')\n",
        "  train_encoded = train_encoded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test encoded\n",
        "test_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_encoded_v3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_wo0daVy1uF"
      },
      "source": [
        "### Filter documents and labels where labels are binary ('true' or 'false')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra0Ovldew6xw"
      },
      "source": [
        "train_labels_lst = train['label-liar'].tolist()\n",
        "train_encoded_lst = train_encoded['full_text_encoded'].tolist()\n",
        "\n",
        "test_labels_lst = test['label-liar'].tolist()\n",
        "test_encoded_lst = test_encoded['full_text_encoded'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xiVFSx_MPAS"
      },
      "source": [
        "is_binary_train_index = lambda i: True if train_labels_lst[i] in ('true', 'false') else False\n",
        "\n",
        "is_binary_test_index = lambda i: True if test_labels_lst[i] in ('true', 'false') else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75-G16swtggY"
      },
      "source": [
        "train_labels_binary = []\n",
        "train_encoded_binary = []\n",
        "\n",
        "test_labels_binary = []\n",
        "test_encoded_binary = []\n",
        "\n",
        "for i in range(len(train_labels_lst)):\n",
        "  if is_binary_train_index(i):\n",
        "    train_labels_binary.append(train_labels_lst[i])\n",
        "    train_encoded_binary.append(train_encoded_lst[i])\n",
        "\n",
        "for i in range(len(test_labels_lst)):\n",
        "  if is_binary_test_index(i):\n",
        "    test_labels_binary.append(test_labels_lst[i])\n",
        "    test_encoded_binary.append(test_encoded_lst[i])\n",
        "\n",
        "# documents encodings are saved as strings\n",
        "# use eval to return it to actual encodings (list of numbers)\n",
        "for i, doc in enumerate(train_encoded_binary):\n",
        "  train_encoded_binary[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_encoded_binary):\n",
        "  test_encoded_binary[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAIdRy2Vlyvt"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-EjLo2Olyvt"
      },
      "source": [
        "train_labels_binary_df = pd.DataFrame({'label-liar': train_labels_binary})\n",
        "test_labels_binary_df = pd.DataFrame({'label-liar': test_labels_binary})\n",
        "\n",
        "train_labels_oh = pd.get_dummies(train_labels_binary_df['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test_labels_binary_df['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWZyW1nlyvu"
      },
      "source": [
        "### Truncate/pad sequences to a max sequence length\n",
        "First we need to determine the value of max sequence length.<br>\n",
        "Find minimum, maximum and average sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FoYxA9blyvu",
        "outputId": "1c45dcaa-8da3-40b0-bbdf-5beaea4b4d04"
      },
      "source": [
        "doc_len = len(train_encoded_binary[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in train_encoded_binary:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 38\n",
            "max sequence length = 1616\n",
            "avrage sequence length = 393.13873830976917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyKPF5Ujlyvv"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8OpvCs31bXc"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzOaFACelyvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425f6b6d-8d53-49cd-e002-3a4a5b7d3d01"
      },
      "source": [
        "maxlen = 500 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in tqdm(train_encoded_binary, position=0, leave=True):\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in tqdm(test_encoded_binary, position=0, leave=True):\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5330/5330 [00:00<00:00, 36780.76it/s]\n",
            "100%|██████████| 457/457 [00:00<00:00, 52527.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML_BKUtulyvv",
        "outputId": "5a5a0027-7cda-479a-d3e0-d603e2a14a20"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{500}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB70rLWrlyvv"
      },
      "source": [
        "### Transformer blocks and positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8qPdzamlyvv"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHuVyP6Clyvw"
      },
      "source": [
        "def positional_embeddings(maxlen):\n",
        "  pe_lst = []\n",
        "  for pos in range(maxlen):\n",
        "    pe_lst.append(sin(pos))\n",
        "\n",
        "  return pe_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKhN0gzZlyvw"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, maxlen):\n",
        "        # positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = tf.convert_to_tensor(positional_embeddings(maxlen))\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI2kJnAJlyvw"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oNTqZ1nlyvw"
      },
      "source": [
        "vocab_size = 56668 # taken from 'Fake news embedding' notebook\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs, maxlen)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhR9C-A0lyvw"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jm9XO9glyvw",
        "outputId": "ffb5e0e5-6f95-4a9c-de4f-40979b673984"
      },
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"binary_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=4, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "134/134 [==============================] - 84s 613ms/step - loss: 0.6728 - binary_accuracy: 0.5896 - recall_21: 0.5896 - val_loss: 0.6239 - val_binary_accuracy: 0.6492 - val_recall_21: 0.6492\n",
            "Epoch 2/4\n",
            "134/134 [==============================] - 82s 609ms/step - loss: 0.5881 - binary_accuracy: 0.6694 - recall_21: 0.6694 - val_loss: 0.6350 - val_binary_accuracy: 0.6426 - val_recall_21: 0.6426\n",
            "Epoch 3/4\n",
            "134/134 [==============================] - 83s 617ms/step - loss: 0.3781 - binary_accuracy: 0.8416 - recall_21: 0.8416 - val_loss: 0.5850 - val_binary_accuracy: 0.7298 - val_recall_21: 0.7298\n",
            "Epoch 4/4\n",
            "134/134 [==============================] - 83s 622ms/step - loss: 0.2067 - binary_accuracy: 0.9230 - recall_21: 0.9230 - val_loss: 0.7676 - val_binary_accuracy: 0.7186 - val_recall_21: 0.7186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHrZgpLBlyvx"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwTNeIxflyvw"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n77DVTMYlyvx",
        "outputId": "fd7a2987-2514-4085-8f83-a350b27ee029"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 3s 178ms/step - loss: 0.8222 - binary_accuracy: 0.6958 - recall_21: 0.6958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwt-4QEWL0m4"
      },
      "source": [
        "## Model 3 - Binary classification, merged classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPKS3na5L0m_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6aSE_VaL0nA"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzE7-wA1L0nA"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train encoded\n",
        "num_splits = 4\n",
        "train_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_encoded_v3_{i}.csv')\n",
        "  train_encoded = train_encoded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test encoded\n",
        "test_encoded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_encoded_v3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yY3esxOOdfg"
      },
      "source": [
        "train_encoded_lst = train_encoded['full_text_encoded'].tolist()\n",
        "test_encoded_lst = test_encoded['full_text_encoded'].tolist()\n",
        "\n",
        "# documents encodings are saved as strings\n",
        "# use eval to return it to actual encodings (list of numbers)\n",
        "for i, doc in enumerate(train_encoded_lst):\n",
        "  train_encoded_lst[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_encoded_lst):\n",
        "  test_encoded_lst[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA2kzy7eL0nA"
      },
      "source": [
        "### Merge labels\n",
        "- ('true', 'mostly-true', 'half-true') => 'true'\n",
        "- ('false', 'pants-fire', 'barely-true') => 'false'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BBCOM7ZMbEQ",
        "outputId": "89857667-a6a6-41d6-8d4d-406ef9282cf2"
      },
      "source": [
        "train['label-liar'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['barely-true', 'pants-fire', 'half-true', 'mostly-true', 'true',\n",
              "       'false'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84KivsUZOZsv"
      },
      "source": [
        "merge_labels = lambda label: 'true' if label in ('true', 'mostly-true', 'half-true') else 'false'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-okja-L0nA"
      },
      "source": [
        "train_labels_lst = train['label-liar'].tolist()\n",
        "test_labels_lst = test['label-liar'].tolist()\n",
        "\n",
        "train_labels_merged = []\n",
        "test_labels_merged = []\n",
        "\n",
        "for label in train_labels_lst:\n",
        "  train_labels_merged.append(merge_labels(label))\n",
        "\n",
        "for label in test_labels_lst:\n",
        "  test_labels_merged.append(merge_labels(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnsZAsBnL0nB"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSwcTLPeL0nB"
      },
      "source": [
        "train_labels_merged_df = pd.DataFrame({'label-liar': train_labels_merged})\n",
        "test_labels_merged_df = pd.DataFrame({'label-liar': test_labels_merged})\n",
        "\n",
        "train_labels_oh = pd.get_dummies(train_labels_merged_df['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test_labels_merged_df['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxt6tc0L0nC"
      },
      "source": [
        "### Truncate/pad sequences to a max sequence length\n",
        "First we need to determine the value of max sequence length.<br>\n",
        "Find minimum, maximum and average sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqQe8zejL0nC",
        "outputId": "2dffb0ff-8ff3-4201-c8a1-e51e1355e617"
      },
      "source": [
        "doc_len = len(train_encoded_lst[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in train_encoded_lst:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 33\n",
            "max sequence length = 1616\n",
            "avrage sequence length = 466.2858263541781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SErOwniL0nC"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5OjmeYKL0nD"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1xoqQB2L0nD"
      },
      "source": [
        "maxlen = 500 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in train_encoded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in test_encoded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Sb9FKGL0nD",
        "outputId": "9c191cc6-3716-453d-b378-01f7f45c74a5"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in test_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{500}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klgOzYJkL0nD"
      },
      "source": [
        "### Transformer blocks and positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF3qjNZyL0nD"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJg8N1LL0nD"
      },
      "source": [
        "def positional_embeddings(maxlen):\n",
        "  pe_lst = []\n",
        "  for pos in range(maxlen):\n",
        "    pe_lst.append(sin(pos))\n",
        "\n",
        "  return pe_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lxzLqasL0nE"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, maxlen):\n",
        "        # positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = tf.convert_to_tensor(positional_embeddings(maxlen))\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fl0a929L0nE"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9iS3aE7L0nE"
      },
      "source": [
        "vocab_size = 56668 # taken from 'Fake news embedding' notebook\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs, maxlen)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Pe-W4CL0nE"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMiaXVWBL0nE",
        "outputId": "129e9688-ca4f-449c-8718-3ba9ddd411ed"
      },
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"binary_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=1, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "377/377 [==============================] - 239s 628ms/step - loss: 0.6877 - binary_accuracy: 0.5449 - recall_22: 0.5449 - val_loss: 0.5836 - val_binary_accuracy: 0.6828 - val_recall_22: 0.6828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHyGasTeL0nE"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88d_E_mHL0nE"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY-zm9z0L0nE",
        "outputId": "c040cbba-3565-43e0-8ddd-b2b1ada985ee"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 7s 184ms/step - loss: 0.6209 - binary_accuracy: 0.6532 - recall_22: 0.6532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMrm6kjKTEv"
      },
      "source": [
        "# Learning with Bert's contextualized embeddings\n",
        "\n",
        "In this section, we use the fake news embeddings we extracted from Bert as data for our model.<br>\n",
        "We extracted two different embeddings:<br>\n",
        "**The first**, embeddings Bert learned with *max sequence length* of 128.<br>\n",
        "**The second**, embeddings Bert learned with *max sequence length* of 512 (the max *max sequence length* for bert).<br>\n",
        "<br>\n",
        "We will build, fit and evaluate three different models:\n",
        "1. Transformer model that **multiclass-classifies** to **6** categories, fitted on **the first** embeddings.\n",
        "2. Transformer model that **multiclass-classifies** to **6** categories, fitted on **the second** embeddings.\n",
        "3. Transformer model that **binary-classifies**, fitted on **the second** embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qHHQvF7Tjtu"
      },
      "source": [
        "## Model 1 - Multiclass-classification, *max sequence length* = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw6fG_PmAKzI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jZqZ8WyG_nQ"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZI06eOI3tCS"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train embedded\n",
        "num_splits = 4\n",
        "train_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_{i}.csv')\n",
        "  train_embedded = train_embedded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test embedded\n",
        "test_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_embedded.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "a62uZSxkWFPe",
        "outputId": "2ef0e0e5-ae57-438c-f294-3a9dc5b480f3"
      },
      "source": [
        "train_embedded.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text_embedded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-0.39664504, -3.201748, -1.6849961, 0.911999...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-0.112607814, -2.6428027, -1.084317, 1.01488...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[1.1767617, -1.7852252, -2.0916524, 0.0093422...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  full_text_embedded\n",
              "0  [[-0.39664504, -3.201748, -1.6849961, 0.911999...\n",
              "1  [[-0.112607814, -2.6428027, -1.084317, 1.01488...\n",
              "2  [[1.1767617, -1.7852252, -2.0916524, 0.0093422..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IduuSFCD5ZcR"
      },
      "source": [
        "train_embedded_lst = train_embedded['full_text_embedded'].tolist()\n",
        "test_embedded_lst = test_embedded['full_text_embedded'].tolist()\n",
        "\n",
        "# documents embeddings are saved as strings\n",
        "# use eval to return it to acctual embeddings (list of word embedding vectors)\n",
        "for i, doc in enumerate(train_embedded_lst):\n",
        "  train_embedded_lst[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_embedded_lst):\n",
        "  test_embedded_lst[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ltfdPPHLUk"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quwQ9eyYmYtd"
      },
      "source": [
        "train_labels_oh = pd.get_dummies(train['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAy41GcLMozM"
      },
      "source": [
        "### Truncate/pad sequences to a *max sequence length*\n",
        "\n",
        "First we need to determine the value of max sequence length.<br>\n",
        "Find minimum, maximum and average sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FWMV3gPvRGW",
        "outputId": "3a09002d-1528-4865-8da6-ad42f1d00d34"
      },
      "source": [
        "doc_len = len(train_embedded_lst[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in train_embedded_lst:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 49\n",
            "max sequence length = 109\n",
            "avrage sequence length = 81.47448309689022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4h6UNa5voOh"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 80."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzqRJB5NmJ"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xk8nC_5DTK"
      },
      "source": [
        "maxlen = 80 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in train_embedded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in test_embedded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKouqeIMQdYL",
        "outputId": "a605903c-a107-411f-f4a8-fba29b7df97f"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in test_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{80}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_ZhoM26V0l6"
      },
      "source": [
        "### Add positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2FAq4IEPy0B"
      },
      "source": [
        "def positional_embeddings(vector, dims):\n",
        "  dim1, dim2 = dims\n",
        "  pe = 0\n",
        "  for pos in range(dim1):\n",
        "    for i in range(dim2):\n",
        "\n",
        "      if i%2 == 0: # is even\n",
        "        pe = sin(pos/10000**(i/dim2))\n",
        "\n",
        "      else: # is odd\n",
        "        pe = cos(pos/10000**((i-1)/dim2))\n",
        "\n",
        "      vector[pos][i] += pe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSchG_APV0HZ"
      },
      "source": [
        "word_size = 5 # each word is represented as a vector with dim = word_size\n",
        "\n",
        "for vector in train_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))\n",
        "\n",
        "for vector in test_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQxvIqsgNG4e"
      },
      "source": [
        "### Transformer block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9wDYcDDdDxT"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgs-H2c6NT6k"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECOBfkt7iFXR"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,5))\n",
        "embedding_layer = layers.Dense(embed_dim) # (not realy an embedding_layer, regular fc layer)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttwDOj8yh1kg"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgZV_pbRkgy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8150d3d0-99de-4ada-b4d1-652fe1319407"
      },
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"categorical_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "377/377 [==============================] - 25s 63ms/step - loss: 1.7809 - categorical_accuracy: 0.1985 - recall_23: 2.4773e-05 - val_loss: 1.7370 - val_categorical_accuracy: 0.2308 - val_recall_23: 0.0000e+00\n",
            "Epoch 2/2\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.7439 - categorical_accuracy: 0.2171 - recall_23: 2.0010e-04 - val_loss: 1.7263 - val_categorical_accuracy: 0.2371 - val_recall_23: 0.0013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPPwKsy-FhBV"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpO3vnuM0Z1t"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0lV9sKTFdT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648bada5-e87c-48d0-fdae-5739207acd87"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 1s 20ms/step - loss: 1.7545 - categorical_accuracy: 0.2014 - recall_23: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsIHfrbpTvUh"
      },
      "source": [
        "## Model 2 - Multiclass-classification, *max sequence length* = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhbdIEDAy6W1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONsf-wSxT_yj"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcSwJYVvT_yr"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train embedded\n",
        "num_splits = 12\n",
        "train_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_v2_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_v2_{i}.csv')\n",
        "  train_embedded = train_embedded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test embedded\n",
        "test_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_embedded.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "RJ-jGh9jT_ys",
        "outputId": "493773eb-bf12-43cf-df6b-1b91580ac670"
      },
      "source": [
        "train_embedded.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text_embedded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-0.37441736, -4.348481, -2.867022, -0.833148...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-1.0242298, -3.5270042, -1.9221768, 0.864446...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.03841588, -3.0448728, -2.48761, 0.37428683...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  full_text_embedded\n",
              "0  [[-0.37441736, -4.348481, -2.867022, -0.833148...\n",
              "1  [[-1.0242298, -3.5270042, -1.9221768, 0.864446...\n",
              "2  [[0.03841588, -3.0448728, -2.48761, 0.37428683..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kCVRt0PT_yt"
      },
      "source": [
        "train_embedded_lst = train_embedded['full_text_embedded'].tolist()\n",
        "test_embedded_lst = test_embedded['full_text_embedded'].tolist()\n",
        "\n",
        "# documents embeddings are saved as strings\n",
        "# use eval to return it to acctual embeddings (list of word embedding vectors)\n",
        "for i, doc in enumerate(train_embedded_lst):\n",
        "  train_embedded_lst[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_embedded_lst):\n",
        "  test_embedded_lst[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaBSVHV8T_ys"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404LNLIfT_yt"
      },
      "source": [
        "train_labels_oh = pd.get_dummies(train['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T44jTI0NT_yt"
      },
      "source": [
        "### truncate/pad sequences to a *max sequence length*\n",
        "\n",
        "First we need to determine the value of *max sequence length*.<br>\n",
        "Find minimum, maximum and average sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj0bTQ9OT_yu",
        "outputId": "d40ccd79-3917-4eb3-a401-ecda1415e540"
      },
      "source": [
        "doc_len = len(test_embedded_lst[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in test_embedded_lst:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 68\n",
            "max sequence length = 102\n",
            "avrage sequence length = 83.20663866249106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1k4gOjfT_yu"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 230."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgrJeAw_T_yu"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNXY5RPAT_yu"
      },
      "source": [
        "maxlen = 230 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in train_embedded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in test_embedded_lst:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lPXDPlTT_yu",
        "outputId": "15418858-4a01-43dd-f0a5-03ac81d7fd26"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in test_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{230}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh2TyQV6T_yv"
      },
      "source": [
        "### Add positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIXRJetVT_yv"
      },
      "source": [
        "def positional_embeddings(vector, dims):\n",
        "  dim1, dim2 = dims\n",
        "  pe = 0\n",
        "  for pos in range(dim1):\n",
        "    for i in range(dim2):\n",
        "\n",
        "      if i%2 == 0: # is even\n",
        "        pe = sin(pos/10000**(i/dim2))\n",
        "\n",
        "      else: # is odd\n",
        "        pe = cos(pos/10000**((i-1)/dim2))\n",
        "\n",
        "      vector[pos][i] += pe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF652iQoT_yv"
      },
      "source": [
        "word_size = 5 # each word is represented as a vector with dim = word_size\n",
        "\n",
        "for vector in train_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))\n",
        "\n",
        "for vector in test_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqw74DXyT_yv"
      },
      "source": [
        "### Transformer block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32hziRaVT_yv"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veJBBWIMT_yw"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mir-Z-jpT_yw"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,5))\n",
        "embedding_layer = layers.Dense(embed_dim) # regular fc layer\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf1GWZRoT_yv"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtFS_icaT_yw",
        "outputId": "d69bc753-38c9-49ba-93a1-528e1f51bd7b"
      },
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"categorical_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "377/377 [==============================] - 113s 294ms/step - loss: 1.7654 - categorical_accuracy: 0.2194 - recall_24: 0.0011 - val_loss: 1.7134 - val_categorical_accuracy: 0.2547 - val_recall_24: 0.0017\n",
            "Epoch 2/2\n",
            "377/377 [==============================] - 113s 300ms/step - loss: 1.7246 - categorical_accuracy: 0.2507 - recall_24: 0.0057 - val_loss: 1.7146 - val_categorical_accuracy: 0.2617 - val_recall_24: 0.0169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKxhmcwZT_yw"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jstizpyDT_yw"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg3BEYPtT_yw",
        "outputId": "62c86356-a7e5-485b-fae6-f0b4e4dc7d21"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 4s 96ms/step - loss: 2.4300 - categorical_accuracy: 0.1643 - recall_24: 0.1619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvspyYkFYwge"
      },
      "source": [
        "## Model 3 - Binary classification, *max sequence length* = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDVzJLey7I5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from math import sin, cos\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6dQcZ90Ywgg"
      },
      "source": [
        "### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTR--OolYwgh"
      },
      "source": [
        "#train\n",
        "num_splits = 7\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/liar_train_{i}.csv')\n",
        "  train = train.append(temp)\n",
        "\n",
        "#train embedded\n",
        "num_splits = 12\n",
        "train_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_v2_0.csv')\n",
        "for i in range(1, num_splits):\n",
        "  temp = pd.read_csv(f'https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/train_embedded_v2_{i}.csv')\n",
        "  train_embedded = train_embedded.append(temp)\n",
        "\n",
        "#test\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset-train/main/liar_test.csv')\n",
        "\n",
        "#test embedded\n",
        "test_embedded = pd.read_csv('https://raw.githubusercontent.com/AlonBrul/liar-liar-dataset/main/test_embedded.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "yl-2qxG0Ywgh",
        "outputId": "c2917573-b5f2-42f3-b26c-bd33a6021624"
      },
      "source": [
        "train_embedded.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text_embedded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-0.37441736, -4.348481, -2.867022, -0.833148...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-1.0242298, -3.5270042, -1.9221768, 0.864446...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.03841588, -3.0448728, -2.48761, 0.37428683...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  full_text_embedded\n",
              "0  [[-0.37441736, -4.348481, -2.867022, -0.833148...\n",
              "1  [[-1.0242298, -3.5270042, -1.9221768, 0.864446...\n",
              "2  [[0.03841588, -3.0448728, -2.48761, 0.37428683..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 364
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoAoi9pCZLIS"
      },
      "source": [
        "### Filter documents and labels where labels are binary ('true', 'false')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro9h4IFDZLIT"
      },
      "source": [
        "train_labels_lst = train['label-liar'].tolist()\n",
        "train_embedded_lst = train_embedded['full_text_embedded'].tolist()\n",
        "\n",
        "test_labels_lst = test['label-liar'].tolist()\n",
        "test_embedded_lst = test_embedded['full_text_embedded'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA1NrHJWZLIT"
      },
      "source": [
        "is_binary_train_index = lambda i: True if train_labels_lst[i] in ('true', 'false') else False\n",
        "\n",
        "is_binary_test_index = lambda i: True if test_labels_lst[i] in ('true', 'false') else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYPeHEVxZLIT"
      },
      "source": [
        "train_labels_binary = []\n",
        "train_embedded_binary = []\n",
        "\n",
        "test_labels_binary = []\n",
        "test_embedded_binary = []\n",
        "\n",
        "for i in range(len(train_labels_lst)):\n",
        "  if is_binary_train_index(i):\n",
        "    train_labels_binary.append(train_labels_lst[i])\n",
        "    train_embedded_binary.append(train_embedded_lst[i])\n",
        "\n",
        "for i in range(len(test_labels_lst)):\n",
        "  if is_binary_test_index(i):\n",
        "    test_labels_binary.append(test_labels_lst[i])\n",
        "    test_embedded_binary.append(test_embedded_lst[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMLIkurLYwgi"
      },
      "source": [
        "# documents embeddings are saved as strings\n",
        "# use eval to return it to acctual embeddings (list of word embedding vectors)\n",
        "for i, doc in enumerate(train_embedded_binary):\n",
        "  train_embedded_binary[i] = eval(doc)\n",
        "\n",
        "for i, doc in enumerate(test_embedded_binary):\n",
        "  test_embedded_binary[i] = eval(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GYhFWzyZLIU"
      },
      "source": [
        "### Get train and test labels One-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm-c9xAqZLIU"
      },
      "source": [
        "train_labels_binary_df = pd.DataFrame({'label-liar': train_labels_binary})\n",
        "test_labels_binary_df = pd.DataFrame({'label-liar': test_labels_binary})\n",
        "\n",
        "train_labels_oh = pd.get_dummies(train_labels_binary_df['label-liar'])\n",
        "test_labels_oh = pd.get_dummies(test_labels_binary_df['label-liar'])\n",
        "\n",
        "train_labels = np.asarray(train_labels_oh).tolist()\n",
        "test_labels = np.asarray(test_labels_oh).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdKropZYwgj"
      },
      "source": [
        "### truncate/pad sequences to a *max sequence length*\n",
        "\n",
        "First we need to determine the value of *max sequence length*.<br>\n",
        "Find minimum, maximum and average sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59g8-4lMYwgj",
        "outputId": "43c15170-4ee5-41ac-f4f5-dddad707fa54"
      },
      "source": [
        "doc_len = len(train_embedded_binary[0])\n",
        "min_len = doc_len\n",
        "max_len = doc_len\n",
        "avg_len = doc_len\n",
        "for doc in train_embedded_binary:\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  if min_len > doc_len:\n",
        "    min_len = doc_len\n",
        "\n",
        "  elif max_len < doc_len:\n",
        "    max_len = doc_len\n",
        "  \n",
        "  avg_len = (avg_len+doc_len)/2\n",
        "\n",
        "print('min sequence length =', min_len)\n",
        "print('max sequence length =', max_len)\n",
        "print('avrage sequence length =', avg_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min sequence length = 65\n",
            "max sequence length = 291\n",
            "avrage sequence length = 234.99282043613502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E2E3_l2Ywgj"
      },
      "source": [
        "Decision: Our model will use a *max sequence length* of 230"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QHNUehvYwgj"
      },
      "source": [
        "def pad_list(lst, value=0, size=0):\n",
        "  count=0\n",
        "  while count < size:\n",
        "    lst.append(value)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKRfVASYwgk"
      },
      "source": [
        "maxlen = 230 # max sequence length\n",
        "\n",
        "train_docs = []\n",
        "for doc in train_embedded_binary:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  train_docs.append(truncated_doc)\n",
        "\n",
        "test_docs = []\n",
        "for doc in test_embedded_binary:\n",
        "  truncated_doc = doc[:maxlen] # truncate to max len\n",
        "  pad_size = maxlen - len(truncated_doc)\n",
        "  pad_list(truncated_doc, value=[0]*5, size=pad_size) # pad end of seq with zeros\n",
        "  test_docs.append(truncated_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VWKExywYwgk",
        "outputId": "5ab3832b-129b-47ea-8166-72ab19a088ee"
      },
      "source": [
        "# check that all sequence lengths' are the same\n",
        "doc_lens = set()\n",
        "for doc in train_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "for doc in test_docs:\n",
        "  doc_lens.add(len(doc))\n",
        "\n",
        "print(doc_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{230}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfdrzJy-Ywgk"
      },
      "source": [
        "### Add positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC58mdDLYwgk"
      },
      "source": [
        "def positional_embeddings(vector, dims):\n",
        "  dim1, dim2 = dims\n",
        "  pe = 0\n",
        "  for pos in range(dim1):\n",
        "    for i in range(dim2):\n",
        "\n",
        "      if i%2 == 0: # is even\n",
        "        pe = sin(pos/10000**(i/dim2))\n",
        "\n",
        "      else: # is odd\n",
        "        pe = cos(pos/10000**((i-1)/dim2))\n",
        "\n",
        "      vector[pos][i] += pe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ZADRiYYwgk"
      },
      "source": [
        "word_size = 5 # each word is represented as a vector with dim = word_size\n",
        "\n",
        "for vector in train_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))\n",
        "\n",
        "for vector in test_docs:\n",
        "  positional_embeddings(vector, (maxlen, word_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i7npX-OYwgk"
      },
      "source": [
        "### Transformer block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfTxwwpiYwgl"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhSBt2leYwgl"
      },
      "source": [
        "### Build model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGSNFidGYwgl"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,5))\n",
        "embedding_layer = layers.Dense(embed_dim) # (not realy an embedding_layer, regular fc layer)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo0MPN1DYwgl"
      },
      "source": [
        "x_train = tf.convert_to_tensor(train_docs)\n",
        "y_train = tf.convert_to_tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzVkZojbYwgl",
        "outputId": "21669d5d-2795-4707-a85e-8e37c3e591da"
      },
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"binary_accuracy\", tf.keras.metrics.Recall()])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_split=0.2, shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "134/134 [==============================] - 42s 302ms/step - loss: 0.6756 - binary_accuracy: 0.5956 - recall_25: 0.5956 - val_loss: 0.6406 - val_binary_accuracy: 0.6407 - val_recall_25: 0.6407\n",
            "Epoch 2/2\n",
            "134/134 [==============================] - 39s 295ms/step - loss: 0.6559 - binary_accuracy: 0.6173 - recall_25: 0.6173 - val_loss: 0.6389 - val_binary_accuracy: 0.6576 - val_recall_25: 0.6576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlXJhbwFYwgl"
      },
      "source": [
        "### Evaluate model on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Ybcdp1Ywgl"
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_docs)\n",
        "y_test = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKCk66UqYwgl",
        "outputId": "657c056c-4891-4621-cbaf-1bce036d7e68"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 97ms/step - loss: 0.7092 - binary_accuracy: 0.4902 - recall_25: 0.4902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_GdHxSEfiW9"
      },
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Fake_news_encoding.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}